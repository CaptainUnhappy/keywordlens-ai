
import os
import requests
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Optional

# ==================== é…ç½® ====================
# è¿™é‡Œåº”è¯¥æœ€å¥½ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–ï¼Œæš‚æ—¶ä¿ç•™åŸæ¥ demo çš„é»˜è®¤å€¼
ZHIPU_API_KEY = "REDACTED_ZHIPU_KEY"
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/embeddings"
EMBEDDING_DIMENSIONS = 2048

def get_embedding(texts: List[str]) -> np.ndarray:
    """
    è°ƒç”¨æ™ºè°±AI APIè·å–æ–‡æœ¬å‘é‡ï¼ˆè‡ªåŠ¨åˆ†æ‰¹å¤„ç†ï¼‰
    """
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json",
    }

    BATCH_SIZE = 64 # æ ¹æ®è¦æ±‚è°ƒæ•´ä¸ºæœ€å¤§å€¼64
    all_embeddings = []

    # åˆ†æ‰¹å¤„ç†
    total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(texts), BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch_texts = texts[i : i + BATCH_SIZE]
        
        # ç®€å•è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        batch_texts = [t for t in batch_texts if t.strip()]
        if not batch_texts:
            continue

        print(f"ğŸ“¡ ZhipuAI Embedding (Batch {batch_num}/{total_batches}, Size: {len(batch_texts)})...")

        data = {
            "model": "embedding-3",
            "input": batch_texts,
            "dimensions": EMBEDDING_DIMENSIONS,
        }

        try:
            response = requests.post(ZHIPU_API_URL, headers=headers, json=data, timeout=30)
            
            if response.status_code != 200:
                print(f"âŒ API Error: {response.status_code} - {response.text}")
                # é‡åˆ°é”™è¯¯å¡«å……é›¶å‘é‡ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))
                continue

            result = response.json()
            
            if "data" not in result:
                 print(f"âŒ API Format Error: {result}")
                 all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))
                 continue

            # æå–å‘é‡
            embeddings = [item["embedding"] for item in result["data"]]
            all_embeddings.extend(embeddings)

        except Exception as e:
            print(f"âŒ Request Exception: {e}")
            all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))

    return np.array(all_embeddings)

def score_keywords(keywords: List[str], product_description: str) -> List[Dict]:
    """
    è®¡ç®—å…³é”®è¯ä¸äº§å“æè¿°çš„ç›¸ä¼¼åº¦åˆ†æ•°
    """
    if not keywords or not product_description:
        return []

    print(f"ğŸ“ Calculating scores for {len(keywords)} keywords against description...")
    
    # 1. è·å–äº§å“å‘é‡
    try:
        product_vec = get_embedding([product_description])[0]
    except Exception as e:
        print(f"âŒ Failed to embed product description: {e}")
        return [{"keyword": kw, "score": 0.0} for kw in keywords]

    # 2. è·å–å…³é”®è¯å‘é‡
    keyword_vecs = get_embedding(keywords)
    
    if len(keyword_vecs) == 0:
        return []

    # 3. è®¡ç®—ç›¸ä¼¼åº¦
    try:
        # Reshape for scikit-learn: (n_samples, n_features)
        product_vec_reshaped = product_vec.reshape(1, -1)
        similarities = cosine_similarity(product_vec_reshaped, keyword_vecs)[0]
    except Exception as e:
        print(f"âŒ Cosine similarity calculation failed: {e}")
        return [{"keyword": kw, "score": 0.0} for kw in keywords]

    # 4. æ ¼å¼åŒ–ç»“æœ
    results = []
    for i, kw in enumerate(keywords):
        score = float(similarities[i])
        results.append({
            "keyword": kw,
            "score": score
        })
    
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    results.sort(key=lambda x: x["score"], reverse=True)
    return results
