
import os
import requests
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Optional, Callable
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# ==================== é…ç½® ====================
# è·å– API KEY
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")
if not ZHIPU_API_KEY:
    # Try one level up if script is run from scripts dir
    load_dotenv(os.path.join(os.path.dirname(__file__), '../../.env'))
    ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")

if not ZHIPU_API_KEY:
    print("Warning: ZHIPU_API_KEY not found in environment variables.")
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/embeddings"
EMBEDDING_DIMENSIONS = 2048

def get_embedding(texts: List[str], batch_size: int = 64, progress_callback: Optional[Callable[[int], None]] = None) -> np.ndarray:
    """
    è°ƒç”¨æ™ºè°±AI APIè·å–æ–‡æœ¬å‘é‡ï¼ˆè‡ªåŠ¨åˆ†æ‰¹å¤„ç†ï¼‰
    æ³¨ï¼šè¿™é‡Œçš„ progress_callback ä¸»è¦ç”¨äºå•çº¯çš„ä¸€ç»„æ–‡æœ¬åŠæ—¶çš„å†…éƒ¨åé¦ˆï¼Œ
    ä½†åœ¨ score_keywords é‡Œæˆ‘ä»¬ä¼šè‡ªå·±æ§åˆ¶æ›´å¤æ‚çš„è¿›åº¦é€»è¾‘ï¼Œæ‰€ä»¥è¿™é‡Œé»˜è®¤ä¸ä¼  callback ä¹Ÿå¯ä»¥ã€‚
    """
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json",
    }

    all_embeddings = []

    # åˆ†æ‰¹å¤„ç†
    total_batches = (len(texts) + batch_size - 1) // batch_size

    for i in range(0, len(texts), batch_size):
        batch_num = i // batch_size + 1
        batch_texts = texts[i : i + batch_size]
        
        # ç®€å•è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        batch_texts = [t for t in batch_texts if t.strip()]
        if not batch_texts:
            continue

        # print(f"ğŸ“¡ ZhipuAI Embedding (Batch {batch_num}/{total_batches}, Size: {len(batch_texts)})...")

        data = {
            "model": "embedding-3",
            "input": batch_texts,
            "dimensions": EMBEDDING_DIMENSIONS,
        }

        try:
            response = requests.post(ZHIPU_API_URL, headers=headers, json=data, timeout=30)
            
            if response.status_code != 200:
                print(f"âŒ API Error: {response.status_code} - {response.text}")
                # é‡åˆ°é”™è¯¯å¡«å……é›¶å‘é‡ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))
                continue

            result = response.json()
            
            if "data" not in result:
                 print(f"âŒ API Format Error: {result}")
                 all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))
                 continue

            # æå–å‘é‡
            embeddings = [item["embedding"] for item in result["data"]]
            all_embeddings.extend(embeddings)

        except Exception as e:
            print(f"âŒ Request Exception: {e}")
            all_embeddings.extend([np.zeros(EMBEDDING_DIMENSIONS)] * len(batch_texts))
        
        # Call internal batch callback if needed
        # if progress_callback:
        #     progress_callback(len(all_embeddings))

    return np.array(all_embeddings)

def score_keywords(
    keywords: List[str], 
    product_description: str,
    progress_callback: Optional[Callable[[int, str], None]] = None
) -> List[Dict]:
    """
    è®¡ç®—å…³é”®è¯ä¸äº§å“æè¿°çš„ç›¸ä¼¼åº¦åˆ†æ•°
    
    progress_callback: func(percent: int, message: str)
    """
    if not keywords or not product_description:
        if progress_callback:
            progress_callback(100, "No input data.")
        return []

    if progress_callback:
        progress_callback(0, "Analyzing product info...")
    
    print(f"ğŸ“ Calculating scores for {len(keywords)} keywords against description...")
    
    # 1. è·å–äº§å“å‘é‡ (10% è¿›åº¦)
    try:
        product_vec = get_embedding([product_description])[0]
        if progress_callback:
            progress_callback(10, "Product analysis complete. Starting keywords...")
    except Exception as e:
        print(f"âŒ Failed to embed product description: {e}")
        if progress_callback:
            progress_callback(100, f"Error: {str(e)}")
        return [{"keyword": kw, "score": 0.0} for kw in keywords]

    # 2. è·å–å…³é”®è¯å‘é‡ (æ‰¹æ¬¡å¤„ç†)
    # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ‹†è§£è¿™ä¸€æ­¥ä»¥ä¾¿æ±‡æŠ¥è¿›åº¦
    keyword_vecs = []
    
    BATCH_SIZE = 64
    total_keywords = len(keywords)
    total_batches = (total_keywords + BATCH_SIZE - 1) // BATCH_SIZE
    
    processed_count = 0
    
    # é‡æ–°å®ç°åˆ†æ‰¹è°ƒç”¨ä»¥ä¾¿æ’å…¥ progress_callback
    # åŸ get_embedding æ¯”è¾ƒé€šç”¨ï¼Œè¿™é‡Œä¸ºäº†è¿›åº¦æ¡ç²¾ç»†æ§åˆ¶ï¼Œæ‰‹åŠ¨å¾ªç¯è°ƒç”¨
    
    for i in range(0, total_keywords, BATCH_SIZE):
        batch_slice = keywords[i : i + BATCH_SIZE]
        if not batch_slice:
            continue
            
        current_batch_vecs = get_embedding(batch_slice, batch_size=BATCH_SIZE)
        keyword_vecs.extend(current_batch_vecs)
        
        processed_count += len(batch_slice)
        
        # è®¡ç®—è¿›åº¦
        # 0-10% æ˜¯äº§å“æè¿°
        # 10%-100% æ˜¯å…³é”®è¯
        # current = 10 + (processed / total) * 90
        percent = 10 + int((processed_count / total_keywords) * 90)
        # é™åˆ¶æœ€å¤§ 99ï¼Œç­‰æœ€åä¸€æ­¥æ‰ 100
        if percent >= 100: percent = 99
        
        print(f"DEBUG: Processed {processed_count}/{total_keywords}, Percent: {percent}%")

        if progress_callback:
            batch_num = i // BATCH_SIZE + 1
            progress_callback(percent, f"Analyzing keywords batch {batch_num}/{total_batches}...")

    keyword_vecs = np.array(keyword_vecs)
    
    if len(keyword_vecs) == 0:
        return []

    # 3. è®¡ç®—ç›¸ä¼¼åº¦
    try:
        # Reshape for scikit-learn: (n_samples, n_features)
        product_vec_reshaped = product_vec.reshape(1, -1)
        similarities = cosine_similarity(product_vec_reshaped, keyword_vecs)[0]
    except Exception as e:
        print(f"âŒ Cosine similarity calculation failed: {e}")
        return [{"keyword": kw, "score": 0.0} for kw in keywords]

    # 4. æ ¼å¼åŒ–ç»“æœ
    results = []
    for i, kw in enumerate(keywords):
        if i < len(similarities):
             score = float(similarities[i])
        else:
             score = 0.0 # Should not happen if vecs match
             
        results.append({
            "keyword": kw,
            "score": score
        })
    
    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    results.sort(key=lambda x: x["score"], reverse=True)
    
    if progress_callback:
        progress_callback(100, "Analysis Complete.")
        
    return results
