#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–å…³é”®è¯è¿‡æ»¤æµç¨‹ï¼š
1. ä½¿ç”¨ MCP AI åˆ†æäº§å“å›¾ç‰‡ç”Ÿæˆæè¿°
2. ä½¿ç”¨æ™ºè°± AI Embedding è¿‡æ»¤å…³é”®è¯

éœ€è¦åœ¨ Claude Code ç¯å¢ƒä¸­è¿è¡Œä»¥è®¿é—® MCP å·¥å…·

ä½¿ç”¨æ–¹æ³•:
    python auto_filter_with_ai.py <product_image.jpg> <keywords.xlsx> [--threshold 0.6]
"""

import json
import sys
import os
import argparse
import requests
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from pathlib import Path


# ==================== MCP åˆ†ææç¤ºè¯ ====================

MCP_ANALYSIS_PROMPT = """Analyze this product image and provide a comprehensive description optimized for semantic keyword matching.

Include:
1. Product category (e.g., headband, earbuds, backpack)
2. Visual features (colors, materials, textures, shape)
3. Purpose/use case (occasions, target audience)
4. Style/theme (e.g., festive, minimalist, holiday-specific)
5. Key distinguishing features
6. Related search terms (both generic and specific)

Provide a detailed, flowing description using varied vocabulary that covers different ways people might search for this product. Focus on objective, observable features.
"""


# ==================== é…ç½® ====================

# æ™ºè°±AI APIé…ç½®
ZHIPU_API_KEY = "REDACTED_ZHIPU_KEY"
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/embeddings"
EMBEDDING_DIMENSIONS = 1024


# ==================== æ­¥éª¤ 1: MCP å›¾ç‰‡åˆ†æ ====================


def analyze_product_with_mcp(image_path: str) -> str:
    """
    ä½¿ç”¨ MCP AI åˆ†æäº§å“å›¾ç‰‡

    è¿™ä¸ªå‡½æ•°åœ¨ Claude Code ç¯å¢ƒä¸­ä¼šè°ƒç”¨çœŸå®çš„ MCP å·¥å…·
    åœ¨æ™®é€š Python ç¯å¢ƒä¸­ä¼šè¿”å› Noneï¼Œéœ€è¦æ‰‹åŠ¨æä¾›æè¿°

    Args:
        image_path: äº§å“å›¾ç‰‡è·¯å¾„

    Returns:
        AI ç”Ÿæˆçš„äº§å“æè¿°ï¼ˆåœ¨ Claude Code ç¯å¢ƒä¸­ï¼‰
        Noneï¼ˆåœ¨æ™®é€š Python ç¯å¢ƒä¸­ï¼‰
    """
    print(f"\nğŸ¤– æ­¥éª¤ 1/3: ä½¿ç”¨ MCP AI åˆ†æäº§å“å›¾ç‰‡...")
    print(f"   å›¾ç‰‡: {image_path}")

    # åœ¨ Claude Code ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè¢«è‡ªåŠ¨æ›¿æ¢ä¸ºçœŸå®çš„ MCP è°ƒç”¨
    # ç”¨æˆ·éœ€è¦ç¡®ä¿åœ¨ Claude Code ä¸­è¿è¡Œæ­¤è„šæœ¬

    # æç¤ºç”¨æˆ·
    print(f"\nâš ï¸  æ­¤è„šæœ¬éœ€è¦åœ¨ Claude Code ç¯å¢ƒä¸­è¿è¡Œ")
    print(f"   è¯·å°†ä»¥ä¸‹å†…å®¹å‘é€ç»™ Claude Code:\n")
    print(f"   'ä½¿ç”¨ mcp__zai-mcp-server__analyze_image åˆ†æ {image_path}'")
    print(f"   'ä½¿ç”¨æç¤ºè¯: {MCP_ANALYSIS_PROMPT[:100]}...'")

    return None


def load_product_description_from_json(json_file: str = "product_description.json") -> str:
    """ä» JSON æ–‡ä»¶åŠ è½½äº§å“æè¿°"""
    if not os.path.exists(json_file):
        return None

    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data.get('description', '')


def create_product_description_interactive(image_path: str) -> str:
    """äº¤äº’å¼åˆ›å»ºäº§å“æè¿°ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    print(f"\nğŸ“ è¯·æä¾›äº§å“æè¿°ï¼ˆç”¨äºè¯­ä¹‰åŒ¹é…ï¼‰:")
    print(f"   å‚è€ƒå›¾ç‰‡: {image_path}\n")

    description_parts = []

    # åŸºæœ¬ä¿¡æ¯
    category = input("1. äº§å“ç±»åˆ« (e.g., headband, shoes, earbuds): ").strip()
    if category:
        description_parts.append(f"This is a {category}")

    # é¢œè‰²
    colors = input("2. ä¸»è¦é¢œè‰² (e.g., green, blue, black): ").strip()
    if colors:
        description_parts.append(f"featuring {colors} color")

    # ä¸»é¢˜/é£æ ¼
    theme = input("3. ä¸»é¢˜/é£æ ¼ (e.g., St. Patrick's Day, minimalist, sports): ").strip()
    if theme:
        description_parts.append(f"with {theme} theme")

    # ç”¨é€”
    occasion = input("4. ä½¿ç”¨åœºåˆ (e.g., party, daily use, sports): ").strip()
    if occasion:
        description_parts.append(f"suitable for {occasion}")

    # å…³é”®ç‰¹å¾
    features = input("5. å…³é”®ç‰¹å¾ (e.g., shamrock, wireless, waterproof): ").strip()
    if features:
        description_parts.append(f"Key features: {features}")

    description = ". ".join(description_parts) + "."

    print(f"\nâœ“ ç”Ÿæˆçš„æè¿°:\n{description}\n")

    return description


# ==================== æ­¥éª¤ 2: å…³é”®è¯åŠ è½½ ====================


def load_keywords_from_excel(file_path: str, column_name: str = "å…³é”®è¯") -> list:
    """ä» Excel æ–‡ä»¶åŠ è½½å…³é”®è¯"""
    print(f"\nğŸ“‚ æ­¥éª¤ 2/3: åŠ è½½å…³é”®è¯...")
    print(f"   æ–‡ä»¶: {file_path}")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° Excel æ–‡ä»¶: {file_path}")

    df = pd.read_excel(file_path)

    if column_name not in df.columns:
        raise ValueError(
            f"Excel ä¸­æ²¡æœ‰åˆ— '{column_name}'ï¼Œå¯ç”¨åˆ—: {list(df.columns)}"
        )

    keywords = df[column_name].dropna().str.strip().tolist()
    keywords = [kw for kw in keywords if kw]

    print(f"   âœ“ åŠ è½½äº† {len(keywords)} ä¸ªå…³é”®è¯")

    return keywords


# ==================== æ­¥éª¤ 3: æ™ºè°± AI è¯­ä¹‰è¿‡æ»¤ ====================


def get_embedding(texts: list) -> np.ndarray:
    """è°ƒç”¨æ™ºè°± AI API è·å–æ–‡æœ¬å‘é‡ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼‰"""
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json",
    }

    BATCH_SIZE = 64
    all_embeddings = []
    total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(texts), BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch_texts = texts[i : i + BATCH_SIZE]

        print(
            f"   ğŸ“¡ è°ƒç”¨æ™ºè°± API (æ‰¹æ¬¡ {batch_num}/{total_batches}, æ•°é‡: {len(batch_texts)})"
        )

        data = {
            "model": "embedding-3",
            "input": batch_texts,
            "dimensions": EMBEDDING_DIMENSIONS,
        }

        response = requests.post(ZHIPU_API_URL, headers=headers, json=data)

        if response.status_code != 200:
            raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status_code}\n{response.text}")

        result = response.json()
        embeddings = [item["embedding"] for item in result["data"]]
        all_embeddings.extend(embeddings)

        usage = result.get("usage", {})
        print(f"      âœ“ æ¶ˆè€— tokens: {usage.get('total_tokens', 'N/A')}")

    return np.array(all_embeddings)


def filter_keywords_with_zhipu(
    keywords: list, product_description: str, threshold: float = 0.6
) -> dict:
    """ä½¿ç”¨æ™ºè°± AI Embedding è¿‡æ»¤å…³é”®è¯"""
    print(f"\nğŸ” æ­¥éª¤ 3/3: ä½¿ç”¨æ™ºè°± AI è¿›è¡Œè¯­ä¹‰è¿‡æ»¤...")
    print(f"   é˜ˆå€¼: {threshold}")
    print(f"   å‘é‡ç»´åº¦: {EMBEDDING_DIMENSIONS}")

    # è·å–äº§å“æè¿°å‘é‡
    print(f"\n   ğŸ“ äº§å“æè¿°é¢„è§ˆ:\n   {product_description[:200]}...\n")
    product_vec = get_embedding([product_description])[0]

    # è·å–å…³é”®è¯å‘é‡
    print(f"\n   ğŸ”„ ç¼–ç å…³é”®è¯...")
    keyword_vecs = get_embedding(keywords)

    # è®¡ç®—ç›¸ä¼¼åº¦
    print(f"\n   ğŸ“Š è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...")
    similarities = cosine_similarity([product_vec], keyword_vecs)[0]

    # æ’åº
    keyword_scores = list(zip(keywords, similarities))
    keyword_scores.sort(key=lambda x: x[1], reverse=True)

    # è¿‡æ»¤
    filtered = [(kw, score) for kw, score in keyword_scores if score >= threshold]

    # ç»Ÿè®¡
    stats = {
        "total": len(keywords),
        "filtered": len(filtered),
        "removed": len(keywords) - len(filtered),
        "filter_rate": 1 - len(filtered) / len(keywords),
        "pass_rate": len(filtered) / len(keywords),
        "avg_score": float(np.mean(similarities)),
        "max_score": float(similarities.max()),
        "min_score": float(similarities.min()),
        "threshold": threshold,
    }

    return {
        "filtered_keywords": [kw for kw, _ in filtered],
        "all_scores": {kw: float(score) for kw, score in keyword_scores},
        "ranked_keywords": keyword_scores,
        "stats": stats,
    }


# ==================== ç»“æœè¾“å‡º ====================


def print_results(result: dict):
    """æ‰“å°æ ¼å¼åŒ–ç»“æœ"""
    stats = result["stats"]
    ranked = result["ranked_keywords"]

    print("\n" + "="*70)
    print("âœ… æ™ºè°± AI è¯­ä¹‰è¿‡æ»¤å®Œæˆ")
    print("="*70)

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
    print(f"   æ€»å…³é”®è¯æ•°:   {stats['total']}")
    print(f"   âœ“ é€šè¿‡ç­›é€‰:   {stats['filtered']} ({stats['pass_rate']:.1%})")
    print(f"   âœ— è¢«è¿‡æ»¤:     {stats['removed']} ({stats['filter_rate']:.1%})")
    print(f"   ç›¸ä¼¼åº¦èŒƒå›´:   {stats['min_score']:.3f} - {stats['max_score']:.3f}")
    print(f"   å¹³å‡åˆ†æ•°:     {stats['avg_score']:.3f}")

    # Top 10
    print(f"\nğŸ† Top 10 ç›¸å…³å…³é”®è¯:")
    for i, (kw, score) in enumerate(ranked[:10], 1):
        status = "âœ“" if score >= stats["threshold"] else "âœ—"
        print(f"   {i:2d}. {status} {score:.4f}  {kw}")

    # è¿‡æ»¤å…³é”®è¯ç¤ºä¾‹
    if stats['removed'] > 0:
        print(f"\nâŒ è¢«è¿‡æ»¤å…³é”®è¯ç¤ºä¾‹ (å‰5ä¸ª):")
        removed = [(kw, s) for kw, s in ranked if s < stats['threshold']]
        for i, (kw, score) in enumerate(removed[:5], 1):
            print(f"   {i}. âœ— {score:.4f}  {kw}")


def save_results(
    result: dict,
    excel_file: str,
    keyword_column: str = "å…³é”®è¯",
    output_file: str = None
):
    """ä¿å­˜ç»“æœåˆ° Excel"""
    if output_file is None:
        name, ext = os.path.splitext(excel_file)
        output_file = f"{name}_filtered{ext}"

    df = pd.read_excel(excel_file)

    # æ·»åŠ å¾—åˆ†åˆ—
    all_scores = result["all_scores"]
    threshold = result["stats"]["threshold"]

    df["ç›¸ä¼¼åº¦å¾—åˆ†"] = df[keyword_column].map(lambda x: all_scores.get(x, np.nan))
    df["çŠ¶æ€"] = df["ç›¸ä¼¼åº¦å¾—åˆ†"].apply(
        lambda x: "âœ“ é€šè¿‡" if not pd.isna(x) and x >= threshold else "âœ— è¿‡æ»¤"
    )

    # æ’åºï¼ˆæŒ‰å¾—åˆ†é™åºï¼‰
    df = df.sort_values("ç›¸ä¼¼åº¦å¾—åˆ†", ascending=False, na_position='last')

    # ä¿å­˜
    df.to_excel(output_file, index=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    # åŒæ—¶ä¿å­˜ JSON
    json_file = output_file.replace(".xlsx", ".json").replace(".xls", ".json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ JSON å·²ä¿å­˜: {json_file}")


# ==================== ä¸»å‡½æ•° ====================


def main():
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–å…³é”®è¯è¿‡æ»¤ï¼šMCPå›¾ç‰‡åˆ†æ + æ™ºè°±AIè¯­ä¹‰è¿‡æ»¤"
    )
    parser.add_argument("image", help="äº§å“å›¾ç‰‡è·¯å¾„")
    parser.add_argument("keywords_excel", help="å…³é”®è¯ Excel æ–‡ä»¶")
    parser.add_argument(
        "--threshold", type=float, default=0.6, help="ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤ 0.6)"
    )
    parser.add_argument(
        "--column", default="å…³é”®è¯", help="Excel ä¸­çš„å…³é”®è¯åˆ—å (é»˜è®¤ 'å…³é”®è¯')"
    )
    parser.add_argument(
        "--description", help="ç›´æ¥æä¾›äº§å“æè¿°ï¼ˆè·³è¿‡ MCP åˆ†æï¼‰"
    )
    parser.add_argument(
        "--description-file", help="ä» JSON æ–‡ä»¶åŠ è½½äº§å“æè¿°"
    )

    args = parser.parse_args()

    print("\n" + "="*70)
    print("ğŸš€ è‡ªåŠ¨åŒ–å…³é”®è¯è¯­ä¹‰è¿‡æ»¤æµç¨‹")
    print("="*70)

    # æ­¥éª¤ 1: è·å–äº§å“æè¿°
    product_description = None

    if args.description:
        # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„æè¿°
        product_description = args.description
        print(f"\nâœ“ ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„äº§å“æè¿°")

    elif args.description_file:
        # ä»æ–‡ä»¶åŠ è½½
        product_description = load_product_description_from_json(args.description_file)
        if product_description:
            print(f"\nâœ“ ä»æ–‡ä»¶åŠ è½½äº§å“æè¿°: {args.description_file}")

    if not product_description:
        # å°è¯• MCP åˆ†æï¼ˆéœ€è¦åœ¨ Claude Code ä¸­è¿è¡Œï¼‰
        print(f"\nâš ï¸  è‡ªåŠ¨åˆ†ææ¨¡å¼éœ€è¦åœ¨ Claude Code ç¯å¢ƒä¸­è¿è¡Œ")
        print(f"   æˆ–ä½¿ç”¨ --description å‚æ•°ç›´æ¥æä¾›æè¿°\n")

        choice = input("æ˜¯å¦æ‰‹åŠ¨è¾“å…¥äº§å“æè¿°? (y/n): ").strip().lower()
        if choice == 'y':
            product_description = create_product_description_interactive(args.image)
        else:
            print("\nâŒ æœªæä¾›äº§å“æè¿°ï¼Œé€€å‡º")
            sys.exit(1)

    # æ­¥éª¤ 2: åŠ è½½å…³é”®è¯
    try:
        keywords = load_keywords_from_excel(args.keywords_excel, args.column)
    except Exception as e:
        print(f"\nâŒ åŠ è½½å…³é”®è¯å¤±è´¥: {e}")
        sys.exit(1)

    # æ­¥éª¤ 3: è¯­ä¹‰è¿‡æ»¤
    try:
        result = filter_keywords_with_zhipu(
            keywords, product_description, args.threshold
        )

        # æ‰“å°ç»“æœ
        print_results(result)

        # ä¿å­˜ç»“æœ
        save_results(result, args.keywords_excel, args.column)

        print("\nâœ… æµç¨‹å®Œæˆï¼")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨è¿‡æ»¤åçš„å…³é”®è¯è¿›è¡Œ Amazon æœç´¢")
        print(f"   é€šè¿‡çš„å…³é”®è¯æ•°: {result['stats']['filtered']}")
        print(f"   é¢„è®¡èŠ‚çœæœç´¢: {result['stats']['filter_rate']:.1%}")

    except Exception as e:
        print(f"\nâŒ è¿‡æ»¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
