#!/usr/bin/env python3
"""
æ‰¹é‡å…³é”®è¯ AI åˆ†æè„šæœ¬

æµç¨‹ï¼š
1. åˆ†æåŸºå‡†äº§å“ï¼ˆç¬¬ä¸€æ­¥ï¼‰
2. AI è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤å¼€å¯ï¼‰
3. æ‰¹é‡æœç´¢äºšé©¬é€Šï¼ˆæµè§ˆå™¨å¤ç”¨ï¼‰
4. å‡†å¤‡ MCP è¯·æ±‚ï¼ˆå¹¶å‘ï¼‰
5. å¹¶å‘ MCP åˆ†æ
"""

import sys
import json
import argparse
import threading
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Set
import pandas as pd

# å¯¼å…¥å•ä¸ªå…³é”®è¯åˆ†æ
from analyze_keyword_with_ai import analyze_reference_product
from search_amazon import AmazonSearcher


def load_keywords_from_excel(excel_file: str, keyword_column: str = "å…³é”®è¯") -> List[str]:
    """ä» Excel åŠ è½½å…³é”®è¯åˆ—è¡¨"""
    df = pd.read_excel(excel_file)

    if keyword_column not in df.columns:
        raise ValueError(f"æœªæ‰¾åˆ°åˆ—: {keyword_column}\nå¯ç”¨åˆ—: {list(df.columns)}")

    keywords = df[keyword_column].dropna().unique().tolist()
    return keywords


def save_filtered_keywords(keywords: List[str], output_file: str):
    """ä¿å­˜è¿‡æ»¤åçš„å…³é”®è¯åˆ°æ–‡æœ¬æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        for kw in keywords:
            f.write(f"{kw}\n")


# ==================== å¹¶å‘å®‰å…¨çš„è¿›åº¦ç®¡ç† ====================

class ProgressTracker:
    """å¹¶å‘å®‰å…¨çš„è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self, cache_file: str):
        self.cache_file = Path(cache_file)
        self.lock = threading.Lock()
        self._data = self._load()

    def _load(self) -> Dict:
        """åŠ è½½è¿›åº¦æ•°æ®"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {
            "completed_folders": [],      # å·²å®Œæˆçš„æ–‡ä»¶å¤¹åˆ—è¡¨
            "current_folder": None,       # å½“å‰å¤„ç†çš„æ–‡ä»¶å¤¹
            "failed_keywords": [],        # å¤±è´¥çš„å…³é”®è¯åˆ—è¡¨
            "mcp_completed": [],          # MCP åˆ†æå®Œæˆçš„æ–‡ä»¶å¤¹
            "mcp_pending": [],            # MCP åˆ†æå¾…å¤„ç†çš„æ–‡ä»¶å¤¹
            "status": "in_progress"       # æ€»ä½“çŠ¶æ€
        }

    def save(self):
        """ä¿å­˜è¿›åº¦æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)

    def add_completed(self, folder_name: str):
        """æ·»åŠ å·²å®Œæˆçš„æ–‡ä»¶å¤¹"""
        with self.lock:
            if folder_name not in self._data["completed_folders"]:
                self._data["completed_folders"].append(folder_name)
            self._data["current_folder"] = folder_name

    def add_mcp_pending(self, folder_name: str):
        """æ·»åŠ åˆ° MCP å¾…å¤„ç†åˆ—è¡¨"""
        with self.lock:
            if folder_name not in self._data["mcp_pending"]:
                self._data["mcp_pending"].append(folder_name)

    def add_mcp_completed(self, folder_name: str):
        """æ·»åŠ åˆ° MCP å·²å®Œæˆåˆ—è¡¨"""
        with self.lock:
            if folder_name in self._data["mcp_pending"]:
                self._data["mcp_pending"].remove(folder_name)
            if folder_name not in self._data["mcp_completed"]:
                self._data["mcp_completed"].append(folder_name)

    def add_failed(self, keyword: str, error: str):
        """æ·»åŠ å¤±è´¥çš„å…³é”®è¯"""
        with self.lock:
            self._data["failed_keywords"].append({"keyword": keyword, "error": error})

    def get_completed_folders(self) -> Set[str]:
        """è·å–å·²å®Œæˆçš„æ–‡ä»¶å¤¹é›†åˆ"""
        with self.lock:
            return set(self._data.get("completed_folders", []))

    def get_mcp_pending(self) -> List[str]:
        """è·å– MCP å¾…å¤„ç†åˆ—è¡¨"""
        with self.lock:
            return self._data.get("mcp_pending", []).copy()

    def set_status(self, status: str):
        """è®¾ç½®æ€»ä½“çŠ¶æ€"""
        with self.lock:
            self._data["status"] = status

    def get_summary(self) -> Dict:
        """è·å–è¿›åº¦æ‘˜è¦"""
        with self.lock:
            return self._data.copy()


# ==================== AI è¿‡æ»¤å…³é”®è¯ ====================

def filter_keywords_with_ai(
    keywords: List[str],
    reference_analysis: Dict,
    product_image: str,
    output_path: Path,
    debug: bool = False
) -> List[str]:
    """
    ä½¿ç”¨ AI è¿‡æ»¤å…³é”®è¯

    åŸºäºåŸºå‡†äº§å“åˆ†æç»“æœï¼Œä½¿ç”¨ AI åˆ¤æ–­å“ªäº›å…³é”®è¯ä¸äº§å“ç›¸å…³

    Args:
        keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨
        reference_analysis: åŸºå‡†äº§å“åˆ†æç»“æœ
        product_image: åŸºå‡†äº§å“å›¾ç‰‡è·¯å¾„
        output_path: è¾“å‡ºç›®å½•
        debug: è°ƒè¯•æ¨¡å¼

    Returns:
        è¿‡æ»¤åçš„å…³é”®è¯åˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print(f"é˜¶æ®µ 1/4: AI è¿‡æ»¤å…³é”®è¯")
    print(f"{'='*60}")
    print(f"åŸå§‹å…³é”®è¯æ•°: {len(keywords)}")
    print(f"{'='*60}\n")

    # ç”Ÿæˆ AI è¿‡æ»¤è¯·æ±‚
    filter_request_file = output_path / "keyword_filter_request.json"

    # æå–äº§å“ç‰¹å¾
    features = reference_analysis.get("features", {})
    feature_text = f"""
é¢œè‰²: {features.get('color', 'æœªçŸ¥')}
é£æ ¼: {features.get('style', 'æœªçŸ¥')}
æè´¨: {features.get('material', 'æœªçŸ¥')}
å½¢çŠ¶: {features.get('shape', 'æœªçŸ¥')}
ç”¨é€”: {features.get('usage', 'æœªçŸ¥')}
å…³é”®è¯: {features.get('keywords', [])}
"""

    # åˆ›å»ºè¿‡æ»¤è¯·æ±‚
    filter_request = {
        "product_image": product_image,
        "product_features": features,
        "original_keywords": keywords,
        "total_keywords": len(keywords),
        "instruction": f"""è¯·åˆ†æè¿™äº›å…³é”®è¯ï¼Œåˆ¤æ–­å“ªäº›ä¸åŸºå‡†äº§å“ç›¸å…³ã€‚

åŸºå‡†äº§å“ç‰¹å¾ï¼š
{feature_text}

è¯·ä»ä»¥ä¸‹å…³é”®è¯ä¸­ç­›é€‰å‡ºä¸åŸºå‡†äº§å“ç›¸å…³çš„å…³é”®è¯ï¼š
{json.dumps(keywords, ensure_ascii=False, indent=2)}

è¿”å› JSON æ ¼å¼ï¼š
{{
  "relevant_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...],
  "irrelevant_keywords": ["å…³é”®è¯3", "å…³é”®è¯4", ...],
  "reasons": {{
    "å…³é”®è¯1": "ç›¸å…³åŸå› ",
    "å…³é”®è¯2": "ç›¸å…³åŸå› "
  }}
}}

ç­›é€‰æ ‡å‡†ï¼š
1. äº§å“çš„æ ¸å¿ƒåŠŸèƒ½æˆ–ç”¨é€”
2. ç›¸åŒæˆ–ç›¸ä¼¼çš„é¢œè‰²/é£æ ¼
3. ç›¸åŒçš„ç›®æ ‡ç”¨æˆ·ç¾¤ä½“
4. äº§å“ç±»å‹æˆ–ç±»åˆ«ç›¸å…³
""",
        "output_file": str(output_path / "filtered_keywords.json")
    }

    # ä¿å­˜è¿‡æ»¤è¯·æ±‚
    with open(filter_request_file, 'w', encoding='utf-8') as f:
        json.dump(filter_request, f, ensure_ascii=False, indent=2)

    print("\nğŸ“‹ AI å…³é”®è¯è¿‡æ»¤è¯·æ±‚å·²ç”Ÿæˆ")
    print(f"è¯·æ±‚æ–‡ä»¶: {filter_request_file}")
    print(f"\n{'='*60}")
    print("è¯·ä½¿ç”¨ä»¥ä¸‹æç¤ºè¿›è¡Œ AI è¿‡æ»¤:")
    print(f"{'='*60}")
    print(f"\nè¯·åˆ†æ {filter_request_file} ä¸­çš„å…³é”®è¯ï¼Œ")
    print("åˆ¤æ–­å“ªäº›ä¸åŸºå‡†äº§å“ç›¸å…³ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°:")
    print(f"  {output_path / 'filtered_keywords.json'}")
    print(f"\n{'='*60}\n")

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿‡æ»¤ç»“æœ
    filtered_file = output_path / "filtered_keywords.json"
    if filtered_file.exists():
        try:
            with open(filtered_file, 'r', encoding='utf-8') as f:
                filtered_data = json.load(f)
                filtered_keywords = filtered_data.get("relevant_keywords", [])

                if filtered_keywords:
                    print(f"âœ“ å·²åŠ è½½è¿‡æ»¤ç»“æœ: {len(filtered_keywords)}/{len(keywords)} ä¸ªç›¸å…³å…³é”®è¯")
                    print(f"  è¿‡æ»¤ç‡: {100 * (1 - len(filtered_keywords)/len(keywords)):.1f}%")

                    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
                    txt_file = output_path / "filtered_keywords.txt"
                    save_filtered_keywords(filtered_keywords, str(txt_file))
                    print(f"âœ“ å·²ä¿å­˜åˆ°: {txt_file}")

                    return filtered_keywords
        except Exception as e:
            print(f"âš  è¯»å–è¿‡æ»¤ç»“æœå¤±è´¥: {e}")

    return []


def prepare_mcp_requests(
    keywords: List[str],
    search_results_cache: Dict,
    product_image: str,
    reference_analysis: Dict,
    output_path: Path,
    grid_columns: int,
    progress: ProgressTracker,
    no_ssl_verify: bool = False,
    debug: bool = False,
    max_workers: int = 5
) -> List[Dict]:
    """
    ä¸ºæ‰€æœ‰å…³é”®è¯å‡†å¤‡ MCP è¯·æ±‚ï¼ˆå¹¶å‘å¤„ç†ï¼‰

    è¿™ä¸ªå‡½æ•°ä¼šï¼š
    1. å¹¶å‘ä¸‹è½½å¹¶åˆå¹¶æ‰€æœ‰å…³é”®è¯çš„å›¾ç‰‡
    2. ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆ MCP è¯·æ±‚æ–‡ä»¶
    3. è¿”å›æ‰€æœ‰å¾…å¤„ç†çš„ MCP ä»»åŠ¡åˆ—è¡¨
    """
    from merge_images import merge_images_grid
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time

    mcp_tasks = []

    print(f"\n{'='*60}")
    print(f"é˜¶æ®µ 3/4: å‡†å¤‡ MCP è¯·æ±‚ï¼ˆå¹¶å‘å¤„ç†ï¼Œ{max_workers}çº¿ç¨‹ï¼‰")
    print(f"{'='*60}\n")

    def process_keyword(keyword_info):
        """å¹¶å‘å¤„ç†å•ä¸ªå…³é”®è¯"""
        idx, keyword = keyword_info
        safe_keyword = keyword.replace(" ", "_").replace("/", "_")[:50]

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æœ
        existing_folders = list(output_path.glob(f"*_{safe_keyword}"))
        if existing_folders:
            keyword_dir = sorted(existing_folders, reverse=True)[0]
            result_file = keyword_dir / "analysis_result.json"
            if result_file.exists():
                return {
                    "status": "skipped",
                    "keyword": keyword,
                    "message": "å·²æœ‰åˆ†æç»“æœ"
                }

        # è·å–æœç´¢ç»“æœ
        if keyword not in search_results_cache or search_results_cache[keyword]["count"] == 0:
            return {
                "status": "skipped",
                "keyword": keyword,
                "message": "æ— æœç´¢ç»“æœ"
            }

        image_urls = search_results_cache[keyword]["image_urls"]

        # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå…³é”®è¯åœ¨å‰ï¼Œä½¿ç”¨å¾®ç§’çº§æ—¶é—´æˆ³é¿å…å†²çªï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        keyword_dir = output_path / f"{safe_keyword}_{timestamp}"
        keyword_dir.mkdir(exist_ok=True)

        # åˆå¹¶å›¾ç‰‡
        merged_path = keyword_dir / "merged_grid.jpg"
        try:
            merge_images_grid(
                image_urls=image_urls,
                output_path=str(merged_path),
                columns=grid_columns,
                img_size=(200, 200),
                debug=debug,
                no_ssl_verify=no_ssl_verify
            )
        except Exception as e:
            progress.add_failed(keyword, f"åˆå¹¶å›¾ç‰‡å¤±è´¥: {e}")
            progress.save()
            return {
                "status": "failed",
                "keyword": keyword,
                "error": str(e)
            }

        # ä¿å­˜æœç´¢ç»“æœ
        json_path = keyword_dir / "search_result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({"keyword": keyword, "image_urls": image_urls, "count": len(image_urls)},
                      f, ensure_ascii=False, indent=2)

        # æ›´æ–°è¿›åº¦
        progress.add_completed(keyword_dir.name)
        progress.save()

        return {
            "status": "success",
            "keyword": keyword,
            "merged_image": str(merged_path),
            "keyword_dir": keyword_dir,
            "image_count": len(image_urls)
        }

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å…³é”®è¯
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_keyword, (i+1, kw)): i+1
                   for i, kw in enumerate(keywords)}

        # æ”¶é›†ç»“æœ
        completed = 0
        for future in as_completed(futures):
            idx = futures[future]
            result = future.result()
            completed += 1

            if result["status"] == "success":
                mcp_tasks.append({
                    "keyword": result["keyword"],
                    "merged_image": result["merged_image"],
                    "keyword_dir": result["keyword_dir"]
                })
                print(f"[{completed}/{len(keywords)}] âœ“ {result['keyword']} ({result['image_count']}å¼ )")
            elif result["status"] == "skipped":
                print(f"[{completed}/{len(keywords)}] â­ {result['keyword']}: {result['message']}")
            elif result["status"] == "failed":
                print(f"[{completed}/{len(keywords)}] âœ— {result['keyword']}: {result['error']}")

    print(f"\nâœ“ å¹¶å‘å¤„ç†å®Œæˆ: æˆåŠŸ {len(mcp_tasks)} ä¸ª")

    return mcp_tasks


def generate_concurrent_mcp_prompts(
    mcp_tasks: List[Dict],
    product_image: str,
    reference_analysis: Dict,
    output_path: Path,
    progress: ProgressTracker
) -> str:
    """
    ç”Ÿæˆå¹¶å‘ MCP è°ƒç”¨çš„æç¤ºä¿¡æ¯

    è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ MCP è¯·æ±‚çš„æç¤ºå­—ç¬¦ä¸²
    """
    print(f"\n{'='*60}")
    print(f"é˜¶æ®µ 4/4: å¹¶å‘ MCP åˆ†ææç¤º")
    print(f"{'='*60}\n")

    # ç”Ÿæˆæ‰¹é‡ MCP è¯·æ±‚æ–‡ä»¶
    batch_mcp_file = output_path / "batch_mcp_requests.json"
    batch_requests = []

    for task in mcp_tasks:
        keyword = task["keyword"]
        merged_image = task["merged_image"]
        keyword_dir = task["keyword_dir"]

        # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆè¯¦ç»†çš„ MCP è¯·æ±‚
        mcp_request = {
            "step": 2,
            "keyword": keyword,
            "image": str(merged_image),
            "tool": "zai-mcp-server__analyze_image",
            "prompt": f"""åˆ†æåˆå¹¶å›¾ä¸­æ‰€æœ‰äº§å“ä¸å‚è€ƒäº§å“çš„ç›¸ä¼¼åº¦ã€‚

å‚è€ƒäº§å“ç‰¹å¾ï¼š
é¢œè‰²: {reference_analysis.get('features', {}).get('color', 'æœªçŸ¥')}
é£æ ¼: {reference_analysis.get('features', {}).get('style', 'æœªçŸ¥')}
æè´¨: {reference_analysis.get('features', {}).get('material', 'æœªçŸ¥')}
å½¢çŠ¶: {reference_analysis.get('features', {}).get('shape', 'æœªçŸ¥')}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "keyword": "{keyword}",
  "products": [
    {{"position": "1-1", "similarity": 0.9, "reason": "...", "recommended": true}},
    ...
  ]
}}""",
            "result_file": str(keyword_dir / "analysis_result.json")
        }

        batch_requests.append(mcp_request)
        progress.add_mcp_pending(keyword_dir.name)

    # ä¿å­˜æ‰¹é‡è¯·æ±‚æ–‡ä»¶
    with open(batch_mcp_file, 'w', encoding='utf-8') as f:
        json.dump({
            "product_image": product_image,
            "reference_analysis": reference_analysis,
            "total_requests": len(batch_requests),
            "requests": batch_requests,
            "timestamp": datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

    progress.save()

    # ç”Ÿæˆæç¤ºä¿¡æ¯
    prompt = f"""
{'='*60}
æ‰¹é‡ MCP å¹¶å‘åˆ†æè¯·æ±‚
{'='*60}

æ€»è®¡ {len(batch_requests)} ä¸ªå…³é”®è¯éœ€è¦ MCP åˆ†æ

æ–¹å¼ 1 - æ‰¹é‡å¹¶å‘è°ƒç”¨ï¼ˆæ¨èï¼‰:
  å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶ç»™ Claudeï¼Œä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è¯·æ±‚ï¼š

  "è¯·å¯¹ {output_path / 'batch_mcp_requests.json'} ä¸­çš„ {len(batch_requests)} ä¸ªå…³é”®è¯è¿›è¡Œå¹¶å‘ MCP åˆ†æ"

æ–¹å¼ 2 - é€ä¸ªè°ƒç”¨:
  æŸ¥çœ‹å„å…³é”®è¯æ–‡ä»¶å¤¹ä¸­çš„ mcp_request.json

{'='*60}
"""

    print(prompt)
    return str(batch_mcp_file)


# ==================== ä¸»æ‰¹å¤„ç†å‡½æ•° ====================

def _load_reference_analysis(output_path: Path, product_image: str, debug: bool) -> Optional[Dict]:
    """åŠ è½½æˆ–åˆ›å»ºåŸºå‡†äº§å“åˆ†æ"""
    ref_path = output_path / "reference_analysis.json"

    if ref_path.exists():
        with open(ref_path, 'r', encoding='utf-8') as f:
            saved_ref = json.load(f)
            if saved_ref.get("analyzed"):
                print("âœ“ å·²åŠ è½½ä¿å­˜çš„åŸºå‡†äº§å“åˆ†æç»“æœ")
                return saved_ref

    # åˆ›å»ºæ–°çš„åˆ†æ
    reference_analysis = analyze_reference_product(product_image, debug)

    if not reference_analysis.get("analyzed"):
        print("\nâš  éœ€è¦å…ˆé€šè¿‡ MCP åˆ†æåŸºå‡†äº§å“")
        print(f"\n{'='*60}")
        print("MCP è°ƒç”¨æç¤º (åŸºå‡†äº§å“åˆ†æ):")
        print(f"{'='*60}")
        print(reference_analysis.get("mcp_prompt", ""))
        print(f"\n{'='*60}")
        print("\nè¯·å®Œæˆä»¥ä¸‹æ­¥éª¤:")
        print("1. ä½¿ç”¨ä¸Šè¿°æç¤ºè°ƒç”¨ MCP: zai-mcp-server__analyze_image")
        print(f"2. å°† MCP è¿”å›çš„ JSON ä¿å­˜åˆ°: {ref_path}")
        print("3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬\n")

        if not ref_path.exists():
            with open(ref_path, 'w', encoding='utf-8') as f:
                json.dump(reference_analysis, f, ensure_ascii=False, indent=2)

        return None

    return reference_analysis


def _filter_keywords_stage(
    keywords: List[str],
    reference_analysis: Dict,
    product_image: str,
    output_path: Path,
    enable_filter: bool,
    debug: bool
) -> List[str]:
    """é˜¶æ®µ1ï¼šAIè¿‡æ»¤å…³é”®è¯"""
    if not enable_filter:
        print("\nâ­ AI è¿‡æ»¤å·²ç¦ç”¨ï¼Œä½¿ç”¨æ‰€æœ‰å…³é”®è¯")
        return keywords

    filtered_keywords = filter_keywords_with_ai(
        keywords=keywords,
        reference_analysis=reference_analysis,
        product_image=product_image,
        output_path=output_path,
        debug=debug
    )

    if not filtered_keywords:
        print("\nâš  æœªæ‰¾åˆ°è¿‡æ»¤ç»“æœï¼Œä½¿ç”¨åŸå§‹å…³é”®è¯")
        return keywords

    print(f"\nâœ“ AI è¿‡æ»¤å®Œæˆ: {len(filtered_keywords)}/{len(keywords)} ä¸ªå…³é”®è¯")

    if not filtered_keywords:
        print("\nâœ— æ²¡æœ‰ç›¸å…³å…³é”®è¯ï¼Œåœæ­¢å¤„ç†")
        return []

    return filtered_keywords


def _batch_search_stage(
    keywords: List[str],
    progress: ProgressTracker,
    output_path: Path,
    amazon_domain: str,
    max_products: int,
    headless: bool,
    debug: bool
) -> Dict[str, Dict]:
    """é˜¶æ®µ2ï¼šæ‰¹é‡æœç´¢Amazon"""
    search_results_cache = {}

    print(f"\n{'='*60}")
    print(f"é˜¶æ®µ 2/4: æ‰¹é‡æœç´¢ (æµè§ˆå™¨å¤ç”¨)")
    print(f"{'='*60}\n")

    completed_folders = progress.get_completed_folders()

    with AmazonSearcher(amazon_domain=amazon_domain, headless=headless, debug=debug) as searcher:
        for i, keyword in enumerate(keywords, 1):
            # æ£€æŸ¥ç¼“å­˜
            safe_keyword = keyword.replace(" ", "_").replace("/", "_")[:50]
            existing_folders = list(output_path.glob(f"*_{safe_keyword}"))

            if existing_folders:
                keyword_dir = sorted(existing_folders, reverse=True)[0]
                result_file = keyword_dir / "analysis_result.json"
                if result_file.exists() and str(keyword_dir.name) in completed_folders:
                    print(f"[{i}/{len(keywords)}] âœ“ ç¼“å­˜: {keyword}")
                    _load_cached_search_result(keyword, keyword_dir, search_results_cache)
                    continue

            # æœç´¢å…³é”®è¯
            _search_single_keyword(
                keyword, i, len(keywords), searcher, max_products,
                search_results_cache, progress
            )

            # å®šæœŸä¼‘æ¯
            if i % 10 == 0 and i < len(keywords):
                print(f"\n  ä¼‘æ¯ 2 ç§’...\n")
                import time
                time.sleep(2)

    return search_results_cache


def _load_cached_search_result(keyword: str, keyword_dir: Path, cache: Dict):
    """åŠ è½½ç¼“å­˜çš„æœç´¢ç»“æœ"""
    search_file = keyword_dir / "search_result.json"
    if search_file.exists():
        try:
            with open(search_file, 'r', encoding='utf-8') as f:
                search_data = json.load(f)
                cache[keyword] = {
                    "count": search_data.get("count", 0),
                    "image_urls": search_data.get("image_urls", [])
                }
        except Exception as e:
            print(f"  âš  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")


def _search_single_keyword(
    keyword: str, index: int, total: int,
    searcher, max_products: int,
    cache: Dict, progress: ProgressTracker
):
    """æœç´¢å•ä¸ªå…³é”®è¯"""
    print(f"[{index}/{total}] ğŸ” æœç´¢: {keyword}")
    try:
        search_result = searcher.search(keyword, max_products=max_products)
        cache[keyword] = search_result

        if search_result["count"] == 0:
            print(f"  âš  æœªæ‰¾åˆ°å•†å“")
            progress.add_failed(keyword, "æœªæ‰¾åˆ°å•†å“")
        else:
            print(f"  âœ“ æ‰¾åˆ° {search_result['count']} ä¸ªå•†å“")
        progress.save()
    except Exception as e:
        print(f"  âœ— æœç´¢å¤±è´¥: {e}")
        progress.add_failed(keyword, str(e))
        progress.save()


def _save_batch_summary(
    output_path: Path,
    keywords: List[str],
    enable_filter: bool,
    search_results_cache: Dict,
    mcp_tasks: List,
    progress: ProgressTracker
):
    """ä¿å­˜æ‰¹å¤„ç†æ±‡æ€»"""
    summary_path = output_path / "batch_summary.json"
    summary_data = progress.get_summary()
    summary_data.update({
        "total_keywords": len(keywords),
        "filtered": enable_filter,
        "searched": len(search_results_cache),
        "prepared_mcp": len(mcp_tasks),
        "timestamp": datetime.now().isoformat()
    })

    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print(f"æ‰¹é‡å¤„ç†å®Œæˆ")
    print(f"{'='*60}")
    print(f"åŸå§‹å…³é”®è¯æ•°: {summary_data.get('total_keywords', 0)}")
    if enable_filter:
        print(f"AI è¿‡æ»¤: å¯ç”¨")
    print(f"æˆåŠŸæœç´¢: {len(search_results_cache)}")
    print(f"å‡†å¤‡ MCP: {len(mcp_tasks)}")
    print(f"å¤±è´¥: {len(summary_data.get('failed_keywords', []))}")
    print(f"æ±‡æ€»å·²ä¿å­˜: {summary_path}")
    print(f"{'='*60}\n")

    if summary_data.get("failed_keywords"):
        print("å¤±è´¥çš„å…³é”®è¯:")
        for item in summary_data["failed_keywords"]:
            print(f"  - {item['keyword']}: {item['error']}")


def batch_analyze(
    keywords: List[str],
    product_image: str,
    amazon_domain: str = "amazon.com",
    max_products: int = 20,
    grid_columns: int = 5,
    similarity_threshold: float = 0.85,
    output_dir: str = "./ai_batch_results",
    cache_file: Optional[str] = None,
    debug: bool = False,
    headless: bool = True,
    no_ssl_verify: bool = False,
    concurrent_workers: int = 5,
    enable_filter: bool = True
) -> List[Dict]:
    """
    é«˜æ•ˆæ‰¹é‡åˆ†æ - å®Œæ•´æµç¨‹

    å››é˜¶æ®µå¤„ç†ï¼š
    1. åˆ†æåŸºå‡†äº§å“ï¼ˆç¬¬ä¸€æ­¥ï¼‰
    2. AI è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
    3. æ‰¹é‡æœç´¢ï¼ˆæµè§ˆå™¨å¤ç”¨ï¼‰
    4. å‡†å¤‡ MCP è¯·æ±‚ï¼ˆå¹¶å‘ä¸‹è½½åˆå¹¶ï¼‰
    5. å¹¶å‘ MCP åˆ†æ

    Args:
        keywords: å…³é”®è¯åˆ—è¡¨
        product_image: åŸºå‡†äº§å“å›¾ç‰‡
        enable_filter: æ˜¯å¦å¯ç”¨ AI è¿‡æ»¤ï¼ˆé»˜è®¤ Trueï¼‰

    Returns:
        list: æ‰€æœ‰åˆ†æç»“æœ
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    cache_file = cache_file or str(output_path / "batch_progress.json")
    progress = ProgressTracker(cache_file)

    print(f"\n{'='*60}")
    print(f"é«˜æ•ˆæ‰¹é‡ AI åˆ†æ (å®Œæ•´æµç¨‹)")
    print(f"{'='*60}")
    print(f"æ€»å…³é”®è¯æ•°: {len(keywords)}")
    print(f"AI è¿‡æ»¤: {'å¯ç”¨' if enable_filter else 'ç¦ç”¨'}")
    print(f"å¹¶å‘æ•°: {concurrent_workers}")
    print(f"{'='*60}\n")

    # é˜¶æ®µ 0: åˆ†æåŸºå‡†äº§å“
    print("=" * 60)
    print("é˜¶æ®µ 0/4: åˆ†æåŸºå‡†äº§å“")
    print("=" * 60)

    reference_analysis = _load_reference_analysis(output_path, product_image, debug)
    if not reference_analysis:
        return []

    # é˜¶æ®µ 1: AI è¿‡æ»¤å…³é”®è¯
    keywords = _filter_keywords_stage(
        keywords, reference_analysis, product_image,
        output_path, enable_filter, debug
    )
    if not keywords:
        return []

    # é˜¶æ®µ 2: æ‰¹é‡æœç´¢
    search_results_cache = _batch_search_stage(
        keywords, progress, output_path, amazon_domain,
        max_products, headless, debug
    )

    # é˜¶æ®µ 3: å‡†å¤‡ MCP è¯·æ±‚
    mcp_tasks = prepare_mcp_requests(
        keywords=keywords,
        search_results_cache=search_results_cache,
        product_image=product_image,
        reference_analysis=reference_analysis,
        output_path=output_path,
        grid_columns=grid_columns,
        progress=progress,
        no_ssl_verify=no_ssl_verify,
        debug=debug,
        max_workers=concurrent_workers
    )

    # é˜¶æ®µ 4: ç”Ÿæˆå¹¶å‘ MCP æç¤º
    if mcp_tasks:
        batch_mcp_file = generate_concurrent_mcp_prompts(
            mcp_tasks=mcp_tasks,
            product_image=product_image,
            reference_analysis=reference_analysis,
            output_path=output_path,
            progress=progress
        )
        progress.set_status("ç­‰å¾…MCPåˆ†æ")
        progress.save()
    else:
        print("\næ‰€æœ‰å…³é”®è¯å·²æœ‰åˆ†æç»“æœæˆ–æ— éœ€å¤„ç†")

    # ä¿å­˜æ±‡æ€»
    _save_batch_summary(output_path, keywords, enable_filter, search_results_cache, mcp_tasks, progress)

    print("\nä¸‹ä¸€æ­¥:")
    print("æŸ¥çœ‹ batch_mcp_requests.jsonï¼Œå¹¶å‘è°ƒç”¨ MCP å®Œæˆåˆ†æ\n")

    return []


# ==================== å‘½ä»¤è¡Œå…¥å£ ====================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å…³é”®è¯ AI åˆ†æ (å®Œæ•´æµç¨‹)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•ï¼ˆå¯ç”¨ AI è¿‡æ»¤ï¼‰
  python batch_analyze_with_ai.py keywords.xlsx product.jpg

  # ç¦ç”¨ AI è¿‡æ»¤ï¼Œä½¿ç”¨æ‰€æœ‰å…³é”®è¯
  python batch_analyze_with_ai.py keywords.xlsx product.jpg --no-filter

  # æŒ‡å®šå…³é”®è¯åˆ—å
  python batch_analyze_with_ai.py keywords.xlsx product.jpg --column "Search Term"

  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python batch_analyze_with_ai.py keywords.xlsx product.jpg -o ./my_results

  # è°ƒè¯•æ¨¡å¼
  python batch_analyze_with_ai.py keywords.xlsx product.jpg --debug
        """
    )

    parser.add_argument('excel_file', help='Excel æ–‡ä»¶è·¯å¾„')
    parser.add_argument('product_image', help='åŸºå‡†äº§å“å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--column', default='å…³é”®è¯', help='å…³é”®è¯åˆ—å (é»˜è®¤: å…³é”®è¯)')
    parser.add_argument('--amazon-domain', default='amazon.com', help='äºšé©¬é€ŠåŸŸå')
    parser.add_argument('--max-products', type=int, default=20, help='æ¯ä¸ªå…³é”®è¯æœ€å¤šè·å–å¤šå°‘å•†å“')
    parser.add_argument('--columns', type=int, default=5, help='ç½‘æ ¼åˆ—æ•°')
    parser.add_argument('--threshold', type=float, default=0.85, help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    parser.add_argument('-o', '--output', default='./ai_batch_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--cache', help='è¿›åº¦ç¼“å­˜æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workers', type=int, default=5, help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 5)')
    parser.add_argument('--no-filter', action='store_true', help='ç¦ç”¨ AI å…³é”®è¯è¿‡æ»¤ï¼ˆä½¿ç”¨æ‰€æœ‰å…³é”®è¯ï¼‰')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--no-headless', action='store_true', help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£')
    parser.add_argument('--no-ssl-verify', action='store_true', help='ç¦ç”¨ SSL éªŒè¯')

    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.excel_file).exists():
        print(f"[é”™è¯¯] Excel æ–‡ä»¶ä¸å­˜åœ¨: {args.excel_file}")
        sys.exit(1)

    if not Path(args.product_image).exists():
        print(f"[é”™è¯¯] äº§å“å›¾ç‰‡ä¸å­˜åœ¨: {args.product_image}")
        sys.exit(1)

    # åŠ è½½å…³é”®è¯
    try:
        keywords = load_keywords_from_excel(args.excel_file, args.column)
        print(f"âœ“ ä» Excel åŠ è½½äº† {len(keywords)} ä¸ªå…³é”®è¯")
    except Exception as e:
        print(f"[é”™è¯¯] åŠ è½½ Excel å¤±è´¥: {e}")
        sys.exit(1)

    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    results = batch_analyze(
        keywords=keywords,
        product_image=args.product_image,
        amazon_domain=args.amazon_domain,
        max_products=args.max_products,
        grid_columns=args.columns,
        similarity_threshold=args.threshold,
        output_dir=args.output,
        cache_file=args.cache,
        debug=args.debug,
        headless=not args.no_headless,
        no_ssl_verify=args.no_ssl_verify,
        concurrent_workers=args.workers,
        enable_filter=not args.no_filter
    )

    if not results:
        print("\n[æç¤º] è¯·å®Œæˆ MCP åˆ†æåé‡æ–°è¿è¡Œä»¥æŸ¥çœ‹ç»“æœ")
        sys.exit(0)


if __name__ == "__main__":
    main()
