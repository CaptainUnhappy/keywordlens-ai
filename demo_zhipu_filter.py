#!/usr/bin/env python3
"""
æ™ºè°±AI Embedding-3 å…³é”®è¯è¿‡æ»¤æ¼”ç¤º
å¯¹æ¯”å›¾ç‰‡äº§å“ä¸å…³é”®è¯åˆ—è¡¨çš„è¯­ä¹‰ç›¸ä¼¼åº¦
"""

import requests
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
from typing import List, Dict
import pandas as pd
import os

# ==================== é…ç½® ====================

# æ™ºè°±AI APIé…ç½®
ZHIPU_API_KEY = "REDACTED_ZHIPU_KEY"
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/embeddings"
EMBEDDING_DIMENSIONS = 2048  # ä½¿ç”¨2048ç»´å‘é‡ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰

# ==================== MCP AI ç”Ÿæˆçš„äº§å“æè¿° ====================
# æ­¤æè¿°ç”± MCP zai-mcp-server çš„ analyze_image å·¥å…·è‡ªåŠ¨ç”Ÿæˆ
# ç”Ÿæˆæ–¹æ³•: åœ¨ Claude Code ä¸­è°ƒç”¨
#   mcp__zai-mcp-server__analyze_image(
#       image_source="å¾®ä¿¡å›¾ç‰‡_20260104145736.png",
#       prompt="åˆ†æäº§å“ç‰¹å¾ç”¨äºè¯­ä¹‰åŒ¹é…..."
#   )

MCP_GENERATED_DESCRIPTION = """This image showcases a festive headband, specifically designed as a St. Patrick's Day accessory. The product is a vibrant green headband adorned with multiple shiny shamrock (clover) decorations and a fringe of green tinsel-like material, making it instantly recognizable as a holiday-themed hair accessory.

Product Category: This is a headband or hair accessory, designed to be worn on the head.

Visual Features: Bright vivid green color throughout. Materials include flexible plastic/foam band, shiny glittery plastic shamrocks with reflective quality, and thin tinsel-like green fringe. Standard U-shaped headband design with decorative front panel densely packed with shamrock shapes and green fringe.

Purpose/Use Case: Festive hair accessory for St. Patrick's Day celebrations, parties, parades, Irish festivals, and themed events. Target audience includes kids, teens, and adults.

Style/Theme: St. Patrick's Day or Irish themed. Style is festive, party, costume, playful, and decorative.

Key Features: Multiple shiny shamrock decorations, green fringe, shamrock symbolism linking to Irish culture, vibrant consistent green color.

Related Search Terms: headband, hair accessory, St. Patrick's Day headband, shamrock headband, clover headband, green headband, St. Paddy's Day headband, Irish headband, party headband, costume accessory, festive headband, holiday headband, St. Patrick's Day hair accessory, St. Patrick's Day costume."""

# äº§å“ä¿¡æ¯ï¼ˆç”¨äºè¯­ä¹‰åŒ¹é…ï¼‰
PRODUCT_INFO = {
    "description": MCP_GENERATED_DESCRIPTION,
    "generation_method": "MCP AI (zai-mcp-server)",
    "image_source": "å¾®ä¿¡å›¾ç‰‡_20260104145736.png"
}


# Excel æ–‡ä»¶é…ç½®
EXCEL_FILE = "åœ£å¸•å‘ç®.xlsx"
KEYWORD_COLUMN = "å…³é”®è¯"

# ç›¸ä¼¼åº¦é˜ˆå€¼
SIMILARITY_THRESHOLD = 0.6


# ==================== è¾…åŠ©å‡½æ•° ====================


def load_keywords_from_excel(file_path: str, column_name: str) -> List[str]:
    """
    ä» Excel æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯åˆ—

    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        column_name: åˆ—å

    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° Excel æ–‡ä»¶: {file_path}")

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
    df = pd.read_excel(file_path)

    if column_name not in df.columns:
        raise ValueError(
            f"Excel æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åˆ— '{column_name}'ï¼Œå¯ç”¨åˆ—: {list(df.columns)}"
        )

    # æå–å…³é”®è¯ï¼Œå»é™¤ç©ºå€¼å’Œç©ºç™½
    keywords = df[column_name].dropna().str.strip().tolist()
    keywords = [kw for kw in keywords if kw]

    print(f"âœ“ æˆåŠŸè¯»å– {len(keywords)} ä¸ªå…³é”®è¯")
    return keywords


# ==================== æ ¸å¿ƒå‡½æ•° ====================


def get_embedding(texts: List[str]) -> np.ndarray:
    """
    è°ƒç”¨æ™ºè°±AI APIè·å–æ–‡æœ¬å‘é‡ï¼ˆè‡ªåŠ¨åˆ†æ‰¹å¤„ç†ï¼‰

    Args:
        texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼Œæ¯æ‰¹æœ€å¤š64æ¡ï¼‰

    Returns:
        numpyæ•°ç»„ï¼Œshape=(len(texts), dimensions)
    """
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json",
    }

    BATCH_SIZE = 64
    all_embeddings = []

    # åˆ†æ‰¹å¤„ç†
    total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(texts), BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch_texts = texts[i : i + BATCH_SIZE]

        print(
            f"ğŸ“¡ è°ƒç”¨æ™ºè°±AI API (æ‰¹æ¬¡ {batch_num}/{total_batches}, æ•°é‡: {len(batch_texts)})..."
        )

        data = {
            "model": "embedding-3",
            "input": batch_texts,
            "dimensions": EMBEDDING_DIMENSIONS,
        }

        response = requests.post(ZHIPU_API_URL, headers=headers, json=data)

        if response.status_code != 200:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}\n{response.text}")

        result = response.json()

        # æå–å‘é‡
        embeddings = [item["embedding"] for item in result["data"]]
        all_embeddings.extend(embeddings)

        # è®°å½•ä½¿ç”¨æƒ…å†µ
        usage = result.get("usage", {})
        print(f"  âœ“ æ¶ˆè€—tokens: {usage.get('total_tokens', 'N/A')}")

    return np.array(all_embeddings)


def generate_product_description(product_info: Dict) -> str:
    """
    ä»äº§å“ä¿¡æ¯ç”Ÿæˆæè¿°æ–‡æœ¬ï¼ˆåŸºäº MCP AI åˆ†æï¼‰

    Args:
        product_info: äº§å“ä¿¡æ¯å­—å…¸

    Returns:
        äº§å“æè¿°å­—ç¬¦ä¸²
    """
    # ä½¿ç”¨ MCP AI åˆ†æçš„è¯¦ç»†æè¿°
    base_description = product_info.get("description", "")

    # æ·»åŠ  MCP è¯†åˆ«çš„å…³é”®æœç´¢è¯
    keywords = [
        "St. Patrick's Day headband",
        "Shamrock headband",
        "Green headband",
        "Clover headband",
        "Irish headband",
        "Party headband",
        "Costume headband",
        "Festive hair accessory",
        "St. Patrick's Day accessory",
        "Green tinsel headband",
        "Holiday headband",
        "St. Paddy's Day headband",
        "Irish costume accessory",
        "Glitter shamrock headband",
        "Sequined headband",
        "Festival hair accessory",
    ]

    # ç»„åˆæè¿°
    description = f"{base_description} {' '.join(keywords)}"
    return description


def filter_keywords(
    keywords: List[str], product_description: str, threshold: float = 0.5
) -> Dict:
    """
    ä½¿ç”¨æ™ºè°±AI Embeddingè¿‡æ»¤å…³é”®è¯

    Args:
        keywords: å…³é”®è¯åˆ—è¡¨
        product_description: äº§å“æè¿°
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        è¿‡æ»¤ç»“æœå­—å…¸
    """
    # è·å–äº§å“æè¿°å‘é‡
    print(f"\nğŸ“ äº§å“æè¿°: {product_description}\n")
    product_vec = get_embedding([product_description])[0]

    # è·å–å…³é”®è¯å‘é‡
    print(f"\nğŸ”„ ç¼–ç  {len(keywords)} ä¸ªå…³é”®è¯...")
    keyword_vecs = get_embedding(keywords)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...\n")
    similarities = cosine_similarity([product_vec], keyword_vecs)[0]

    # æ’åº
    keyword_scores = list(zip(keywords, similarities))
    keyword_scores.sort(key=lambda x: x[1], reverse=True)

    # è¿‡æ»¤
    filtered = [(kw, score) for kw, score in keyword_scores if score >= threshold]

    # ç»Ÿè®¡
    stats = {
        "total": len(keywords),
        "filtered": len(filtered),
        "removed": len(keywords) - len(filtered),
        "filter_rate": 1 - len(filtered) / len(keywords),
        "pass_rate": len(filtered) / len(keywords),
        "avg_score": float(np.mean(similarities)),
        "max_score": float(similarities.max()),
        "min_score": float(similarities.min()),
        "threshold": threshold,
    }

    return {
        "filtered_keywords": [kw for kw, _ in filtered],
        "all_scores": {kw: float(score) for kw, score in keyword_scores},
        "top_keywords": keyword_scores,
        "stats": stats,
    }


def print_results(result: Dict):
    """æ‰“å°æ ¼å¼åŒ–ç»“æœ"""
    stats = result["stats"]
    top_keywords = result["top_keywords"]

    print("=" * 70)
    print("æ™ºè°±AI Embedding-3 è¯­ä¹‰è¿‡æ»¤ç»“æœ")
    print("=" * 70)

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»å…³é”®è¯æ•°:   {stats['total']}")
    print(f"  é€šè¿‡ç­›é€‰:     {stats['filtered']} ({stats['pass_rate']:.1%})")
    print(f"  è¢«è¿‡æ»¤:       {stats['removed']} ({stats['filter_rate']:.1%})")
    print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼:   {stats['threshold']}")
    print(f"  åˆ†æ•°èŒƒå›´:     {stats['min_score']:.3f} - {stats['max_score']:.3f}")
    print(f"  å¹³å‡åˆ†æ•°:     {stats['avg_score']:.3f}")

    # åˆ†æ•°åˆ†å¸ƒ
    print(f"\nğŸ“ˆ åˆ†æ•°åˆ†å¸ƒ:")
    score_ranges = {
        "ä¼˜ç§€ (0.8-1.0)": [kw for kw, s in top_keywords if s >= 0.8],
        "è‰¯å¥½ (0.6-0.8)": [kw for kw, s in top_keywords if 0.6 <= s < 0.8],
        "ä¸­ç­‰ (0.4-0.6)": [kw for kw, s in top_keywords if 0.4 <= s < 0.6],
        "è¾ƒä½ (0-0.4)": [kw for kw, s in top_keywords if s < 0.4],
    }

    for range_name, kws in score_ranges.items():
        count = len(kws)
        pct = count / stats["total"] * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"  {range_name}: {count:2d} ({pct:5.1f}%) {bar}")

    # è¯¦ç»†åˆ—è¡¨
    print(f"\nğŸ“‹ å…³é”®è¯è¯¦ç»†å¾—åˆ†:")
    print(f"{'æ’å':<5} {'çŠ¶æ€':<5} {'å¾—åˆ†':<8} {'å…³é”®è¯'}")
    print("-" * 70)

    for i, (kw, score) in enumerate(top_keywords, 1):
        status = "âœ“ é€šè¿‡" if score >= stats["threshold"] else "âœ— è¿‡æ»¤"
        color_code = "\033[92m" if score >= stats["threshold"] else "\033[91m"
        reset_code = "\033[0m"

        print(f"{i:<5} {color_code}{status:<5}{reset_code} {score:<8.4f} {kw}")

    print("=" * 70)


def save_results(result: Dict, filename: str = "zhipu_filter_result.json"):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ JSONç»“æœå·²ä¿å­˜: {filename}")


def save_to_excel(
    result: Dict,
    excel_file: str,
    keyword_column: str,
    score_column: str = "ç›¸ä¼¼åº¦å¾—åˆ†",
    status_column: str = "çŠ¶æ€",
    output_file: str = None,
):
    """
    å°†è¿‡æ»¤ç»“æœå†™å›Excelæ–‡ä»¶ï¼Œåœ¨å…³é”®è¯åˆ—å³ä¾§æ’å…¥å¾—åˆ†åˆ—

    Args:
        result: è¿‡æ»¤ç»“æœå­—å…¸
        excel_file: åŸExcelæ–‡ä»¶è·¯å¾„
        keyword_column: å…³é”®è¯åˆ—å
        score_column: å¾—åˆ†åˆ—åï¼ˆé»˜è®¤"ç›¸ä¼¼åº¦å¾—åˆ†"ï¼‰
        status_column: çŠ¶æ€åˆ—åï¼ˆé»˜è®¤"çŠ¶æ€"ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤åœ¨åŸæ–‡ä»¶åååŠ "_result"ï¼‰
    """
    if output_file is None:
        name, ext = os.path.splitext(excel_file)
        output_file = f"{name}_result{ext}"

    # è¯»å–Excelæ–‡ä»¶
    df = pd.read_excel(excel_file)

    # ç¡®ä¿å…³é”®è¯åˆ—å­˜åœ¨
    if keyword_column not in df.columns:
        print(f"âš ï¸  è­¦å‘Š: Excelä¸­æ²¡æœ‰æ‰¾åˆ°'{keyword_column}'åˆ—ï¼Œè·³è¿‡Excelå†™å…¥")
        return

    # åˆ›å»ºå¾—åˆ†å’ŒçŠ¶æ€æ˜ å°„
    all_scores = result["all_scores"]
    threshold = result["stats"]["threshold"]

    # æ·»åŠ å¾—åˆ†åˆ—
    df[score_column] = df[keyword_column].map(lambda x: all_scores.get(x, np.nan))

    # æ·»åŠ çŠ¶æ€åˆ—
    def get_status(score):
        if pd.isna(score):
            return "æ— æ•°æ®"
        return "âœ“ é€šè¿‡" if score >= threshold else "âœ— è¿‡æ»¤"

    df[status_column] = df[score_column].apply(get_status)

    # æ‰¾åˆ°å…³é”®è¯åˆ—çš„ä½ç½®
    col_idx = df.columns.get_loc(keyword_column)

    # é‡æ–°æ’åˆ—åˆ—ï¼šå°†å¾—åˆ†å’ŒçŠ¶æ€åˆ—æ”¾åœ¨å…³é”®è¯åˆ—åé¢
    cols = list(df.columns)
    cols.remove(score_column)
    cols.remove(status_column)
    # åœ¨å…³é”®è¯åˆ—åæ’å…¥å¾—åˆ†åˆ—å’ŒçŠ¶æ€åˆ—
    insert_idx = col_idx + 1
    cols.insert(insert_idx, score_column)
    cols.insert(insert_idx + 1, status_column)
    df = df[cols]

    # ä¿å­˜åˆ°æ–°æ–‡ä»¶
    df.to_excel(output_file, index=False)
    print(f"ğŸ’¾ Excelç»“æœå·²ä¿å­˜: {output_file}")
    print(
        f"   å·²æ’å…¥åˆ—: '{score_column}' å’Œ '{status_column}' åˆ° '{keyword_column}' å³ä¾§"
    )


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»æµç¨‹"""
    print("\n" + "=" * 70)
    print("ğŸ€ St. Patrick's Day å¤´é¥° - å…³é”®è¯è¯­ä¹‰è¿‡æ»¤æ¼”ç¤º")
    print("=" * 70)

    # æ˜¾ç¤ºäº§å“ä¿¡æ¯
    print(f"\nğŸ“¦ äº§å“ä¿¡æ¯:")
    # print(f"  åç§°: {PRODUCT_INFO['name']}")
    # print(f"  ç±»åˆ«: {PRODUCT_INFO['category']}")
    # print(f"  é¢œè‰²: {PRODUCT_INFO['color']}")
    # print(f"  é£æ ¼: {PRODUCT_INFO['style']}")
    # print(f"  ç‰¹å¾: {', '.join(PRODUCT_INFO['features'][:3])}...")

    # ä» Excel æ–‡ä»¶åŠ è½½å…³é”®è¯
    try:
        test_keywords = load_keywords_from_excel(EXCEL_FILE, KEYWORD_COLUMN)
    except Exception as e:
        print(f"\nâŒ åŠ è½½å…³é”®è¯å¤±è´¥: {e}")
        return

    print(f"\nğŸ” å¾…æµ‹è¯•å…³é”®è¯æ•°é‡: {len(test_keywords)}")
    print(f"ğŸ“ ç›¸ä¼¼åº¦é˜ˆå€¼: {SIMILARITY_THRESHOLD}")
    print(f"ğŸ¯ å‘é‡ç»´åº¦: {EMBEDDING_DIMENSIONS}")

    # ç”Ÿæˆäº§å“æè¿°
    product_description = generate_product_description(PRODUCT_INFO)

    # è¿‡æ»¤å…³é”®è¯
    try:
        result = filter_keywords(
            keywords=test_keywords,
            product_description=product_description,
            threshold=SIMILARITY_THRESHOLD,
        )

        # æ‰“å°ç»“æœ
        print_results(result)

        # ä¿å­˜ç»“æœåˆ°JSON
        save_results(result)

        # ä¿å­˜ç»“æœåˆ°Excel
        save_to_excel(
            result=result, excel_file=EXCEL_FILE, keyword_column=KEYWORD_COLUMN
        )

        # æˆæœ¬ä¼°ç®—
        total_tokens = result["stats"]["total"] * 5 + 50  # ä¼°ç®—
        cost = (total_tokens / 1_000_000) * 0.5
        print(f"\nğŸ’° é¢„ä¼°æˆæœ¬: Â¥{cost:.6f} (~{total_tokens} tokens)")

        print("\nâœ… è¿‡æ»¤å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
